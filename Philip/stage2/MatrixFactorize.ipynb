{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           User     Book  Rate                       Time          Tag\n",
      "45      1398478  2348372     4  2009-11-10T18:42:00+08:00          NaN\n",
      "164     1779492  1851385     3  2011-03-13T12:37:12+08:00  奥尔罕·帕慕克,土耳其\n",
      "165     1779492  3266345     3  2010-10-20T19:31:20+08:00      葛瑞格·摩顿森\n",
      "166     1779492  1001885     3  2010-10-20T19:29:16+08:00     林达,法国,旅行\n",
      "168     1779492  1424741     3  2010-10-04T01:24:33+08:00      卡森·麦卡勒斯\n",
      "...         ...      ...   ...                        ...          ...\n",
      "637249  4507957  1125186     4  2009-07-04T08:02:13+08:00   张爱玲,半生缘,爱情\n",
      "637250  4507957  1002299     5  2009-07-04T08:01:28+08:00   金庸,武侠,笑傲江湖\n",
      "637251  4507957  1001136     4  2009-07-04T07:55:17+08:00      彼得・潘,童话\n",
      "637252  4507957  1021615     5  2009-07-04T07:53:54+08:00    小王子,童话,经典\n",
      "637253  4507957  1962929     5  2009-06-29T22:13:37+08:00           爱情\n",
      "\n",
      "[403807 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 读loaded_data取保存的 CSV 文件\n",
    "loaded_data = pd.read_csv('data\\\\book_score.csv')\n",
    "\n",
    "# 显示加载的数据\n",
    "#print(loaded_data)\n",
    "loaded_data=loaded_data[loaded_data['Rate'] != 0]\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 处理加载的数据，得到item、user和star信息。\n",
    "#合并相同的userids和itemsids\n",
    "user_ids=loaded_data[\"User\"].unique()\n",
    "item_ids=loaded_data[\"Book\"].unique()\n",
    "#创建user to row的字典\n",
    "user_to_row ={user_id : idx for idx, user_id in enumerate(user_ids)}\n",
    "item_to_row ={item_id : idx for idx, item_id in enumerate(item_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self,data,user_to_row,item_to_row):\n",
    "        self.data=data\n",
    "        #self.user_item_matrix=user_item_matrix\n",
    "        self.user_to_row=user_to_row\n",
    "        self.item_to_row=item_to_row\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        user = self.user_to_row[row['User']]\n",
    "        book = self.item_to_row[row['Book']]\n",
    "        rating = row['Rate'].astype('float32')\n",
    "        return user, book, rating\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim为超参数，由用户定义\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self,num_users,num_books,embedding_dim):\n",
    "        super(MatrixFactorization,self).__init__()\n",
    "        #词嵌入技术，将user和item分别嵌入为向量\n",
    "        self.user_embeddings=nn.Embedding(num_users,embedding_dim)\n",
    "        self.book_embeddings=nn.Embedding(num_books,embedding_dim)\n",
    "\n",
    "    def forward(self, user,book):\n",
    "        # 输出即为user矩阵和book矩阵相乘得到的结果\n",
    "        user_embedding=self.user_embeddings(user)\n",
    "        book_embedding=self.book_embeddings(book)\n",
    "        return (user_embedding*book_embedding).sum(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 创建训练集和测试集的数据集对象和数据加载器\n",
    "train_data, test_data = train_test_split(loaded_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = RatingDataset(train_data, user_to_row, item_to_row)\n",
    "test_dataset = RatingDataset(test_data, user_to_row, item_to_row)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4096, shuffle=False, drop_last = True)\n",
    "\n",
    "embedding_dim=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4312\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "num_users=len(user_ids)\n",
    "num_books=len(item_ids)\n",
    "print(num_users)\n",
    "print(num_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MatrixFactorization(num_users,num_books,embedding_dim).to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 35.65945816040039\n",
      "Epoch 2/10, Loss: 25.75108528137207\n",
      "Epoch 3/10, Loss: 19.289918899536133\n",
      "Epoch 4/10, Loss: 12.165305137634277\n",
      "Epoch 5/10, Loss: 6.186763763427734\n",
      "Epoch 6/10, Loss: 3.435378313064575\n",
      "Epoch 7/10, Loss: 2.5138890743255615\n",
      "Epoch 8/10, Loss: 1.9156748056411743\n",
      "Epoch 9/10, Loss: 1.6768184900283813\n",
      "Epoch 10/10, Loss: 1.4415652751922607\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 10\n",
    "lambda_b=0.001\n",
    "lambda_u=0.001\n",
    "for epoch in range(num_epochs):\n",
    "    for user, book, rating in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user, book)\n",
    "        loss = criterion(output, rating) + lambda_u * model.user_embeddings.weight.norm(2) + lambda_b * model.book_embeddings.weight.norm(2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # 监控损失或其他性能指标\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 2.126078348410757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # 将模型设置为评估模式，不进行梯度更新\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():  # 不计算梯度\n",
    "    for user, book, rating in test_dataloader:\n",
    "        output = model(user, book)\n",
    "        loss = criterion(output, rating)\n",
    "        #print(f\"Test Loss: {loss}\")\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# 计算测试性能指标，例如均方误差或其他指标\n",
    "average_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Average Test Loss: {average_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos similarity=0.9962113396045966\n",
      "NDCG_score=0.8770867967084897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "#使用余弦相似度计算预测顺序和实际数据的差距\n",
    "model.eval()  # 将模型设置为评估模式，不进行梯度更新\n",
    "num_users=int(num_users)\n",
    "num_books=int(num_books)\n",
    "real_score_array=np.zeros((num_users,num_books))\n",
    "pred_score_array=np.zeros((num_users,num_books))\n",
    "\n",
    "with torch.no_grad():  # 不计算梯度\n",
    "    for user, book, rating in test_dataloader:\n",
    "        pred = model(user, book)\n",
    "        for i in range(4096):\n",
    "            real_score_array[int(user[i]),int(book[i])]=int(rating[i])\n",
    "            pred_score_array[int(user[i]),int(book[i])]=float(pred[i])\n",
    "cos_sum=0\n",
    "ndcg_sum=0\n",
    "#抽取前100组user\n",
    "for i in range(100):\n",
    "    #cal ndcg\n",
    "    ndcg=ndcg_score(real_score_array[i].reshape(1,-1),pred_score_array[i].reshape(1,-1))\n",
    "    ndcg_sum=ndcg_sum+ndcg\n",
    "    vec1=np.argsort(real_score_array[i])\n",
    "    vec2=np.argsort(pred_score_array[i])\n",
    "    cos_sim = vec1.dot(vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    cos_sum=cos_sum+cos_sim\n",
    "print(f\"cos similarity={cossum/100}\\nNDCG_score={ndcg_sum/100}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "#print(real_score_array[0])\n",
    "ndcg=ndcg_score(real_score_array[0].reshape(1,-1),pred_score_array[0].reshape(1,-1))\n",
    "print(ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
