{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 763line [00:00, 382759.71line/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 395577070line [1:05:40, 100389.35line/s]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "#将first_jump作为第二跳的起点，保存在second_initial中\n",
    "second_initial = []\n",
    "with gzip.open('first_jump.gz','rb') as f:\n",
    "    for line in tqdm(f, desc=\"Processing lines\", unit=\"line\"):\n",
    "        second_initial.append(line.strip().decode())\n",
    "print(len(second_initial))\n",
    "#开始第二跳\n",
    "str_flag=\"<http://rdf.freebase.com/ns/\"\n",
    "str_fault=\"<http://rdf.freebase.com/ns/people.person.date_of_birth>\"\n",
    "with gzip.open('freebase_douban.gz','rb') as f, gzip.open('output_second_entity.gz','wt') as output1, gzip.open('output_second_3.gz','wt') as output2:\n",
    "    for line in tqdm(f, desc=\"Processing lines\", unit=\"line\"):\n",
    "        triplet = line.strip().decode().split('\\t')[:3]\n",
    "        if triplet[0] in second_initial and str_flag in triplet[2] and str_fault not in triplet[1]:\n",
    "            output1.write(triplet[2]+\"\\n\")\n",
    "            output2.write(triplet[0]+\"\\t\"+triplet[1]+\"\\t\"+triplet[2]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "min_relation = 50  \n",
    "min_entity = 15\n",
    "max_entity = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming douban2fb.txt is a mapping file with movie ID to entity ID\n",
    "mapping_file = 'douban2fb.txt'\n",
    "\n",
    "# Create a set to store the entity IDs from douban2fb.txt for faster lookup\n",
    "entity_ids_list = []\n",
    "movie_id2fb = {}\n",
    "\n",
    "with open(mapping_file, 'r') as mapping:\n",
    "    for line in mapping:\n",
    "        # Assuming the file format is \"movieID\\tentityID\"\n",
    "        movie_id, entity_id = line.strip().split()\n",
    "        entity_id=\"<http://rdf.freebase.com/ns/\" + entity_id+ \">\"\n",
    "        movie_id2fb[movie_id] = entity_id\n",
    "        entity_ids_list.append(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_line(line_to_delete):#line_to_delete是一个list,存储了需要删除的行的行号\n",
    "    with gzip.open('second_3.gz','rb') as second_3:\n",
    "    #删除line_to_deiete中的对应行，保存在temp_3中\n",
    "        with gzip.open('temp_3.gz','wt') as output_temp_3:\n",
    "            line_num = 0\n",
    "            i = 0\n",
    "            num_to_delete = len(line_to_delete)\n",
    "            for line in tqdm(second_3, desc=\"Processing lines\", unit=\"line\"):\n",
    "                if(line_num != line_to_delete[i]):\n",
    "                    output_temp_3.write(line.decode())\n",
    "                else:\n",
    "                    if(i < num_to_delete - 1):\n",
    "                        i = i + 1\n",
    "                line_num = line_num + 1\n",
    "    print(\"task1 successfully!\")\n",
    "    #将temp_3中的内容复制到second_3中\n",
    "    with gzip.open('temp_3.gz','rb') as output_temp_3:\n",
    "        with gzip.open('second_3.gz','wt') as output_second_3:\n",
    "            for line in tqdm(output_temp_3, desc=\"Processing lines\", unit=\"line\"):\n",
    "                output_second_3.write(line.decode())\n",
    "    print(\"task2 successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def update_large(line_to_delete):\n",
    "    with gzip.open('second_3.gz','rb') as second_3:\n",
    "        num_entity = {}\n",
    "        num_relation = {}\n",
    "        for line in tqdm(second_3, desc=\"Processing lines\", unit=\"line\"):\n",
    "            triplet = line.decode().strip().split('\\t')\n",
    "            if triplet[2] in num_entity:\n",
    "                num_entity[triplet[2]] = num_entity[triplet[2]] + 1\n",
    "            else:\n",
    "                num_entity[triplet[2]] = 1\n",
    "            if triplet[0] in num_entity:\n",
    "                num_entity[triplet[0]] = num_entity[triplet[0]] + 1\n",
    "            else:\n",
    "                num_entity[triplet[0]] = 1\n",
    "            if triplet[1] in num_relation:\n",
    "                num_relation[triplet[1]] = num_relation[triplet[1]] + 1\n",
    "            else:\n",
    "                num_relation[triplet[1]] = 1\n",
    "        line_number = 0\n",
    "        second_3.seek(0)\n",
    "        for line in tqdm(second_3, desc=\"Processing lines\", unit=\"line\"):\n",
    "            triplet = line.decode().strip().split('\\t')\n",
    "            if num_entity[triplet[2]] > max_entity and triplet[2] not in entity_ids_list:\n",
    "                line_to_delete.append(line_number)\n",
    "            elif num_entity[triplet[0]] > max_entity and triplet[0] not in entity_ids_list:\n",
    "                line_to_delete.append(line_number)\n",
    "            elif num_relation[triplet[1]] < min_relation:\n",
    "                line_to_delete.append(line_number)\n",
    "            line_number = line_number + 1\n",
    "        print(len(line_to_delete))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 104724704line [04:15, 410481.74line/s]\n",
      "Processing lines: 104724704line [12:42, 137337.57line/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103978969\n"
     ]
    }
   ],
   "source": [
    "line_to_delete = []\n",
    "update_large(line_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103978969\n"
     ]
    }
   ],
   "source": [
    "print(len(line_to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 0line [00:00, ?line/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 104724704line [01:36, 1085590.38line/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 745735line [00:02, 341392.33line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "delete_line(line_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 745735line [00:08, 85811.85line/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745735\n",
      "['<http://rdf.freebase.com/ns/m.0kprc8>', '<http://rdf.freebase.com/ns/film.content_rating.film>', '<http://rdf.freebase.com/ns/m.026hy89>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "second_3 = []\n",
    "with gzip.open('second_3.gz','rb') as f:\n",
    "    for line in tqdm(f,desc = \"Processing lines\",unit = \"line\"):\n",
    "        triplet = line.strip().decode().split('\\t')[:3]\n",
    "        second_3.append(triplet)\n",
    "print(len(second_3))\n",
    "print(second_3[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745735\n"
     ]
    }
   ],
   "source": [
    "print(len(second_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "str = '<http://rdf.freebase.com/ns/m.026pdy>'\n",
    "#统计str在second_3中出现的次数\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(len(second_3)):\n",
    "    if second_3[i][0] == str:\n",
    "        count1 = count1 + 1\n",
    "    elif second_3[i][2] == str:\n",
    "        count2 = count2 + 1\n",
    "print(count1)\n",
    "print(count2)\n",
    "#打印出与之相关的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_relation = 50\n",
    "min_entity = 15\n",
    "max_entity = 20000\n",
    "#函数，通过关系小于50更新second_3，通过second_3更新second_jump\n",
    "def update_second_3_by_relation():\n",
    "    num_relation = {}#存储每个关系的数量\n",
    "    num_of_entity = len(second_3)#实体的数量\n",
    "    for i in range(num_of_entity):\n",
    "        if second_3[i][1] not in num_relation:\n",
    "            num_relation[second_3[i][1]] = 1\n",
    "        else:\n",
    "            num_relation[second_3[i][1]] = num_relation[second_3[i][1]] + 1\n",
    "    id_to_delete = []#存储需要删除的三元组的id\n",
    "    #在second_3中删除关系小于50的三元组,并在second_jump中删除对应的实体\n",
    "    for i in range(num_of_entity):\n",
    "        if num_relation[second_3[i][1]] <= min_relation:\n",
    "            id_to_delete.append(i)\n",
    "    for i in range(len(id_to_delete)):\n",
    "        del second_3[id_to_delete[i]-i]\n",
    "    \n",
    "def update_second_3_by_entity():\n",
    "    num_entity = {}#存储每个实体的数量\n",
    "    num_of_entity = len(second_3)#实体的数量,即三元组的数量\n",
    "    for i in range(num_of_entity):\n",
    "        if second_3[i][0] not in num_entity:\n",
    "            num_entity[second_3[i][0]] = 1\n",
    "        else:\n",
    "            num_entity[second_3[i][0]] = num_entity[second_3[i][0]] + 1\n",
    "        if second_3[i][2] not in num_entity:\n",
    "            num_entity[second_3[i][2]] = 1\n",
    "        else:\n",
    "            num_entity[second_3[i][2]] = num_entity[second_3[i][2]] + 1\n",
    "    id_to_delete = []#存储需要删除的三元组的id\n",
    "    for i in range(num_of_entity):\n",
    "        if num_entity[second_3[i][0]] <= min_entity and second_3[i][0] in entity_ids_list:\n",
    "            continue\n",
    "        elif num_entity[second_3[i][0]] <= min_entity and second_3[i][0] not in entity_ids_list:\n",
    "            id_to_delete.append(i)\n",
    "        elif num_entity[second_3[i][2]] <= min_entity and second_3[i][2] not in entity_ids_list:\n",
    "            id_to_delete.append(i)\n",
    "            \n",
    "    for i in range(len(id_to_delete)):\n",
    "        del second_3[id_to_delete[i]-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理第二跳，删除关系小于50的三元组，删除实体小于15的三元组,轮流执行直至second_3和second_jump不再变化\n",
    "def process_second_jump():\n",
    "    i=0\n",
    "    len_old = len(second_3)\n",
    "    print(i)\n",
    "    print(len_old)\n",
    "    update_second_3_by_entity()\n",
    "    update_second_3_by_relation()\n",
    "    while len_old != len(second_3):\n",
    "        i = i+1\n",
    "        len_old = len(second_3)\n",
    "        print(i)\n",
    "        print(len_old)\n",
    "        update_second_3_by_entity()\n",
    "        update_second_3_by_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "745735\n",
      "1\n",
      "42777\n",
      "2\n",
      "42515\n",
      "3\n",
      "42390\n",
      "4\n",
      "42387\n"
     ]
    }
   ],
   "source": [
    "process_second_jump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42387\n"
     ]
    }
   ],
   "source": [
    "print(len(second_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885\n"
     ]
    }
   ],
   "source": [
    "#判断second_3中的实体是否包含entity_ids_list中的实体\n",
    "entity_in_second_3 = []\n",
    "for i in range(len(second_3)):\n",
    "    entity_in_second_3.append(second_3[i][0])\n",
    "    entity_in_second_3.append(second_3[i][2])\n",
    "\n",
    "entity_in_second_3 = set(entity_in_second_3)\n",
    "print(len(entity_in_second_3))\n",
    "for i in range(len(entity_ids_list)):\n",
    "    if entity_ids_list[i] not in entity_in_second_3:\n",
    "        print(entity_ids_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 42387/42387 [00:00<00:00, 282325.38line/s]\n"
     ]
    }
   ],
   "source": [
    "#将second_3中的内容重新写入second_3.gz中\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "with gzip.open('second_3.gz','wt') as output_second_3:\n",
    "    for i in tqdm(range(len(second_3)),desc = \"Processing lines\",unit = \"line\"):\n",
    "        output_second_3.write(second_3[i][0]+\"\\t\"+second_3[i][1]+\"\\t\"+second_3[i][2]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "1885\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#测试代码\n",
    "#测试正确性\n",
    "num_entity_second_jump = {}\n",
    "num_relation_second_jump = {}\n",
    "num_of_entity = len(second_3)\n",
    "for i in range(num_of_entity):\n",
    "    if second_3[i][0] not in num_entity_second_jump:\n",
    "        num_entity_second_jump[second_3[i][0]] = 1\n",
    "    else:\n",
    "        num_entity_second_jump[second_3[i][0]] = num_entity_second_jump[second_3[i][0]] + 1\n",
    "    if second_3[i][2] not in num_entity_second_jump:\n",
    "        num_entity_second_jump[second_3[i][2]] = 1\n",
    "    else:\n",
    "        num_entity_second_jump[second_3[i][2]] = num_entity_second_jump[second_3[i][2]] + 1\n",
    "    if second_3[i][1] not in num_relation_second_jump:\n",
    "        num_relation_second_jump[second_3[i][1]] = 1\n",
    "    else:\n",
    "        num_relation_second_jump[second_3[i][1]] = num_relation_second_jump[second_3[i][1]] + 1\n",
    "\n",
    "#测试正确性,测试数量是否符合要求\n",
    "min_relation = 50\n",
    "min_entity = 15\n",
    "for i in range(num_of_entity):\n",
    "    if num_entity_second_jump[second_3[i][0]] < min_entity or num_entity_second_jump[second_3[i][0]] > max_entity and second_3[i][0] not in entity_ids_list:\n",
    "        print(\"error\")\n",
    "    elif num_entity_second_jump[second_3[i][2]] < min_entity or num_entity_second_jump[second_3[i][2]] > max_entity and second_3[i][2] not in entity_ids_list:\n",
    "        print(\"error\")\n",
    "    elif num_relation_second_jump[second_3[i][1]] < min_relation and second_3[i][2] not in entity_ids_list and second_3[i][0] not in entity_ids_list:\n",
    "        print(\"error\")\n",
    "\n",
    "print(len(num_entity_second_jump))\n",
    "print(len(num_relation_second_jump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<http://rdf.freebase.com/ns/film.content_rating.film>': 1314, '<http://rdf.freebase.com/ns/type.object.type>': 2683, '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>': 2628, '<http://rdf.freebase.com/ns/film.film.genre>': 1309, '<http://rdf.freebase.com/ns/freebase.valuenotation.is_reviewed>': 1215, '<http://rdf.freebase.com/ns/media_common.netflix_title.netflix_genres>': 3855, '<http://rdf.freebase.com/ns/common.topic.notable_types>': 588, '<http://rdf.freebase.com/ns/film.film.language>': 1037, '<http://rdf.freebase.com/ns/film.film.rating>': 475, '<http://rdf.freebase.com/ns/film.film.country>': 404, '<http://rdf.freebase.com/ns/freebase.valuenotation.has_value>': 77, '<http://rdf.freebase.com/ns/freebase.valuenotation.has_no_value>': 684, '<http://rdf.freebase.com/ns/film.film.production_companies>': 244, '<http://rdf.freebase.com/ns/film.film.prequel>': 63, '<http://rdf.freebase.com/ns/film.film.sequel>': 73, '<http://rdf.freebase.com/ns/type.type.instance>': 5003, '<http://rdf.freebase.com/ns/film.film.film_format>': 139, '<http://rdf.freebase.com/ns/media_common.netflix_genre.titles>': 14637, '<http://rdf.freebase.com/ns/film.film.featured_film_locations>': 74, '<http://rdf.freebase.com/ns/film.film_genre.films_in_this_genre>': 4694, '<http://rdf.freebase.com/ns/kg.object_profile.prominent_type>': 63, '<http://rdf.freebase.com/ns/film.production_company.films>': 682, '<http://rdf.freebase.com/ns/film.film_location.featured_in_films>': 116, '<http://rdf.freebase.com/ns/film.film_format.film_format>': 330}\n"
     ]
    }
   ],
   "source": [
    "print(num_relation_second_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#将num_relation_second_jump写入csv文件\n",
    "with open('num_relation_second_jump.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in num_relation_second_jump.items():\n",
    "        writer.writerow([k,v])\n",
    "#对于csv文件，进行排序，数值从大到小，并且数值列对齐\n",
    "import pandas as pd\n",
    "df = pd.read_csv('num_relation_second_jump.csv',header=None)\n",
    "df.columns = ['relation','number']\n",
    "df = df.sort_values(by='number',ascending=False)\n",
    "df.to_csv('num_relation_second_jump.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#将num_entity_second_jump写入csv文件\n",
    "with open('num_entity_second_jump.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in num_entity_second_jump.items():\n",
    "        writer.writerow([k,v])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
